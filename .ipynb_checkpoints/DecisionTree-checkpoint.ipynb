{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/tljh/user/lib/python3.6/site-packages/sklearn/externals/six.py:31: DeprecationWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
      "  \"(https://pypi.org/project/six/).\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn import metrics \n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'year', 'state_0', 'state_1', 'state_2', 'state_3',\n",
       "       'state_4', 'state_5', 'state_6', 'status_0', 'status_1', 'status_2',\n",
       "       'status_3', 'status_4', 'months', 'pay_method_ACH', 'pay_method_Other',\n",
       "       'pay_method_credit card', 'pay_method_paper check', 'income',\n",
       "       'lp_amount', 'Y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"final_data.csv\", encoding = 'ISO-8859-1')\n",
    "data.head()\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a separate dataframe for the dropped columns that can't be used in a logistic regression\n",
    "data_dropped = data[[ 'months']]\n",
    "\n",
    "# Dropping rows that will not be used in the logistic regression\n",
    "data = data.drop(['Unnamed: 0', 'months'], axis = 1)\n",
    "\n",
    "# Entering the column names into a dataframe\n",
    "col_names = ['year', 'state_0', 'state_1', 'state_2', 'state_3',\n",
    "       'state_4', 'state_5', 'state_6', 'status_0', 'status_1', 'status_2',\n",
    "       'status_3', 'status_4', 'pay_method_ACH', 'pay_method_Other',\n",
    "       'pay_method_credit card', 'pay_method_paper check', 'income',\n",
    "       'lp_amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the features and target variables\n",
    "feature_cols = col_names[:-1]\n",
    "X = data[feature_cols]\n",
    "y = data['Y']\n",
    "\n",
    "# Splitting the data using the train_test_split sklearn package\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random = RandomOverSampler(random_state=2019)\n",
    "X_train_ros, y_train_ros = random.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[21978,  1685],\n",
       "       [ 1247,  1651]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf = clf.fit(X_train_ros ,y_train_ros)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Creating the confusion matrix for the regression\n",
    "cnf_mat2 = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.889612589887429\n",
      "Precision: 0.4949040767386091\n",
      "Recall: 0.5697032436162871\n",
      "F1 Score: 0.8929749051770234\n"
     ]
    }
   ],
   "source": [
    "# Printing the accuracy, precision, recall, and f1 score for the data\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\",metrics.f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.93      0.94     23663\n",
      "           1       0.49      0.57      0.53      2898\n",
      "\n",
      "    accuracy                           0.89     26561\n",
      "   macro avg       0.72      0.75      0.73     26561\n",
      "weighted avg       0.90      0.89      0.89     26561\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing out the classification report for the decision tree\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.969629276373585\n",
      "Test Accuracy: 0.889612589887429\n",
      "Train Precision: 0.7790141622218135\n",
      "Test Precision: 0.4949040767386091\n",
      "Train Recall: 0.9979971724787936\n",
      "Test Recall: 0.5697032436162871\n"
     ]
    }
   ],
   "source": [
    "# Running the trained model on the training and test data to make sure the model is not overfitting\n",
    "\n",
    "# Running accuracy on both sets\n",
    "print(\"Train Accuracy:\",metrics.accuracy_score(y_train, clf.predict(X_train)))\n",
    "print(\"Test Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Running precision on both sets\n",
    "print(\"Train Precision:\",metrics.precision_score(y_train, clf.predict(X_train)))\n",
    "print(\"Test Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# Running recall on both sets\n",
    "print(\"Train Recall:\",metrics.recall_score(y_train, clf.predict(X_train)))\n",
    "print(\"Test Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17534,  6129],\n",
       "       [   88,  2810]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Decision Tree classifer object\n",
    "clf_prune = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "clf_prune = clf_prune.fit(X_train_ros,y_train_ros)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "y_pred = clf_prune.predict(X_test)\n",
    "\n",
    "# Creating the confusion matrix for the regression\n",
    "cnf_mat2 = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.765935017506871\n",
      "Precision: 0.3143528358876832\n",
      "Recall: 0.9696342305037957\n",
      "F1 Score: 0.8085377204060327\n"
     ]
    }
   ],
   "source": [
    "# Printing the accuracy, precision, recall, and f1 score for the data\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\",metrics.f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17534  6129]\n",
      " [   88  2810]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.74      0.85     23663\n",
      "           1       0.31      0.97      0.47      2898\n",
      "\n",
      "    accuracy                           0.77     26561\n",
      "   macro avg       0.65      0.86      0.66     26561\n",
      "weighted avg       0.92      0.77      0.81     26561\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.confusion_matrix(y_test, y_pred))\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.969629276373585\n",
      "Test Accuracy: 0.765935017506871\n",
      "Train Precision: 0.7790141622218135\n",
      "Test Precision: 0.3143528358876832\n",
      "Train Recall: 0.9979971724787936\n",
      "Test Recall: 0.9696342305037957\n"
     ]
    }
   ],
   "source": [
    "# Running the trained model on the training and test data to make sure the model is not overfitting\n",
    "\n",
    "# Running accuracy on both sets\n",
    "print(\"Train Accuracy:\",metrics.accuracy_score(y_train, clf.predict(X_train)))\n",
    "print(\"Test Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Running precision on both sets\n",
    "print(\"Train Precision:\",metrics.precision_score(y_train, clf.predict(X_train)))\n",
    "print(\"Test Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# Running recall on both sets\n",
    "print(\"Train Recall:\",metrics.recall_score(y_train, clf.predict(X_train)))\n",
    "print(\"Test Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
