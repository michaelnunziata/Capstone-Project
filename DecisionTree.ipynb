{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'year', 'state', 'lp_amount', 'status', 'months',\n",
       "       'pay_method_ACH', 'pay_method_Other', 'pay_method_credit card',\n",
       "       'pay_method_paper check', 'region_central', 'region_north_east',\n",
       "       'region_rocky', 'region_south', 'region_south_east', 'region_west',\n",
       "       'status_Active', 'status_Decline', 'status_Returned',\n",
       "       'status_Returned < 90 days', 'status_Returned_90', 'income',\n",
       "       '18_months'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing the necessary packages\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn import metrics \n",
    "\n",
    "# importing packages to visualize the decision tree\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus\n",
    "\n",
    "# Reading in the csv file\n",
    "data = pd.read_csv(\"final_data.csv\", encoding = 'ISO-8859-1')\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows that will not be used in the logistic regression\n",
    "data = data.drop(['Unnamed: 0', 'Unnamed: 0.1', 'months', 'state', 'status', 'pay_method_Other', 'lp_amount'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the X and y variables\n",
    "X = data.iloc[:,:-1]\n",
    "y = data['18_months']\n",
    "\n",
    "# Splitting the data using the train_test_split package\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the RandomOverSampler package to deal with the imbalanced dataset\n",
    "random = RandomOverSampler(random_state=2019)\n",
    "X_train_ros, y_train_ros = random.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[22854,  2061],\n",
       "       [ 1284,  1509]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Decision Tree classifer\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Training the Decision Tree Classifer\n",
    "clf = clf.fit(X_train_ros ,y_train_ros)\n",
    "\n",
    "# Predicting the response for test dataset\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Creating the confusion matrix for the decision tree\n",
    "cnf_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8792767431788653\n",
      "Precision: 0.4226890756302521\n",
      "Recall: 0.5402792696025779\n",
      "F1 Score: 0.8856915063123052\n"
     ]
    }
   ],
   "source": [
    "# Printing the accuracy, precision, recall, and f1 score for the decision tree\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\",metrics.f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.92      0.93     24915\n",
      "           1       0.42      0.54      0.47      2793\n",
      "\n",
      "    accuracy                           0.88     27708\n",
      "   macro avg       0.68      0.73      0.70     27708\n",
      "weighted avg       0.89      0.88      0.89     27708\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing out the classification report for the decision tree\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9631991145651625\n",
      "Test Accuracy: 0.8792767431788653\n",
      "Train Precision: 0.7385981550133632\n",
      "Test Precision: 0.4226890756302521\n",
      "Train Recall: 0.9968582732138701\n",
      "Test Recall: 0.5402792696025779\n"
     ]
    }
   ],
   "source": [
    "# Running the trained model on the training and test data to make sure the model is not overfitting\n",
    "\n",
    "# Running accuracy on both sets\n",
    "print(\"Train Accuracy:\",metrics.accuracy_score(y_train, clf.predict(X_train)))\n",
    "print(\"Test Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Running precision on both sets\n",
    "print(\"Train Precision:\",metrics.precision_score(y_train, clf.predict(X_train)))\n",
    "print(\"Test Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# Running recall on both sets\n",
    "print(\"Train Recall:\",metrics.recall_score(y_train, clf.predict(X_train)))\n",
    "print(\"Test Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19009,  5906],\n",
       "       [  250,  2543]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Decision Tree classifer object to prune the tree\n",
    "# Help to deal with overfitting of the model\n",
    "clf_prune = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n",
    "\n",
    "# Training the  Decision Tree Classifer\n",
    "clf_prune = clf_prune.fit(X_train_ros,y_train_ros)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "y_pred = clf_prune.predict(X_test)\n",
    "\n",
    "# Creating the confusion matrix for the decision tree\n",
    "cnf_mat = metrics.confusion_matrix(y_test, y_pred)\n",
    "cnf_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7778258986574275\n",
      "Precision: 0.30098236477689666\n",
      "Recall: 0.9104905119942714\n",
      "F1 Score: 0.8194917860956958\n"
     ]
    }
   ],
   "source": [
    "# Printing the accuracy, precision, recall, and f1 score for the data\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n",
    "print(\"F1 Score:\",metrics.f1_score(y_test, y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.76      0.86     24915\n",
      "           1       0.30      0.91      0.45      2793\n",
      "\n",
      "    accuracy                           0.78     27708\n",
      "   macro avg       0.64      0.84      0.66     27708\n",
      "weighted avg       0.92      0.78      0.82     27708\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing out the classification report for the decision tree\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy: 0.9631991145651625\n",
      "Test Accuracy: 0.7778258986574275\n",
      "Train Precision: 0.7385981550133632\n",
      "Test Precision: 0.30098236477689666\n",
      "Train Recall: 0.9968582732138701\n",
      "Test Recall: 0.9104905119942714\n"
     ]
    }
   ],
   "source": [
    "# Running the trained model on the training and test data to make sure the model is not overfitting\n",
    "\n",
    "# Running accuracy on both sets\n",
    "print(\"Train Accuracy:\",metrics.accuracy_score(y_train, clf.predict(X_train)))\n",
    "print(\"Test Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Running precision on both sets\n",
    "print(\"Train Precision:\",metrics.precision_score(y_train, clf.predict(X_train)))\n",
    "print(\"Test Precision:\",metrics.precision_score(y_test, y_pred))\n",
    "\n",
    "# Running recall on both sets\n",
    "print(\"Train Recall:\",metrics.recall_score(y_train, clf.predict(X_train)))\n",
    "print(\"Test Recall:\",metrics.recall_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
