{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119833, 15)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reading in each csv file for the years of customer data\n",
    "data = pd.read_csv('data (1) copy.csv', encoding = 'ISO-8859-1')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   #Prod                    Agent Label +ID        Created           ID  \\\n",
      "0    1.0      Church Bulletin 0011 (109398)  7/30/14 14:36  657917124.0   \n",
      "1    1.0  Benefits and More 4 Paid (143595)  10/5/15 19:15  658935850.0   \n",
      "2    1.0  Benefits and More 4 Paid (143595)   5/2/16 20:16  659370939.0   \n",
      "3    1.0  Benefits and More 4 Paid (143595)  4/22/16 19:55  659356995.0   \n",
      "4    1.0       Benefits and More 4 (116063)  7/28/16 16:01  659940549.0   \n",
      "\n",
      "    Income  Last Pay Amount  Last Pay Complete Last Pay Date  Last Pay Status  \\\n",
      "0  36250.0            34.95                1.0       8/25/14              1.0   \n",
      "1  36250.0            29.95                1.0       10/5/15              1.0   \n",
      "2  36250.0            29.95                1.0        5/2/16              1.0   \n",
      "3  36250.0            39.95                1.0       5/12/16              1.0   \n",
      "4  36250.0              NaN                NaN           NaN              NaN   \n",
      "\n",
      "  Last Pay Type  Pay Method State              Status  ZipCodeNew  \\\n",
      "0        Refund    Discover    MA   Returned <30 days      1001.0   \n",
      "1          Sale    Discover    MA  Returned < 90 days      1001.0   \n",
      "2          Sale        Visa    MA      Switched to LW      1001.0   \n",
      "3        Refund  MasterCard    MA   Returned <30 days      1001.0   \n",
      "4           NaN         ACH    ME      Order Canceled      1001.0   \n",
      "\n",
      "               Stage date_created time_created  \n",
      "0  Back in Inventory      7/30/14        14:36  \n",
      "1  Back in Inventory      10/5/15        19:15  \n",
      "2                NaN       5/2/16        20:16  \n",
      "3  Back in Inventory      4/22/16        19:55  \n",
      "4   Post Date Cancel      7/28/16        16:01  \n"
     ]
    }
   ],
   "source": [
    "# Spliting the Created column into two rows with the date and time as seperate columns\n",
    "data['date_created'], data['time_created'] = data['Created'].str.split(' ', 1).str\n",
    "\n",
    "# Printing out the first 5 rows of each column in the dataset\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the two new split columns into datetime objects\n",
    "data['date_created'] = pd.to_datetime(data['date_created'])\n",
    "data['Last Pay Date'] = pd.to_datetime(data['Last Pay Date'])\n",
    "\n",
    "# Taking the difference between the last pay date and the date the account was created \n",
    "# Using the dt.days function to calculate amount of days in between those dates\n",
    "data['days'] = (data['Last Pay Date'] - data['date_created']).dt.days\n",
    "\n",
    "# Dividing the number of days by 30 to get the number of months with the company\n",
    "data['months'] = data['days']/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the year from the date_created column to create a 'year' column\n",
    "data['year'] = pd.DatetimeIndex(data['date_created']).year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rounding up the 'months' and 'days' columns to the hundredth decimal place\n",
    "data['months'] = np.round(data['months'], 2)\n",
    "data['days'] = np.round(data['days'], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119833 entries, 0 to 119832\n",
      "Data columns (total 20 columns):\n",
      "#Prod                119794 non-null float64\n",
      "Agent Label +ID      119832 non-null object\n",
      "Created              119832 non-null object\n",
      "ID                   119832 non-null float64\n",
      "Income               118524 non-null float64\n",
      "Last Pay Amount      113509 non-null float64\n",
      "Last Pay Complete    113509 non-null float64\n",
      "Last Pay Date        113509 non-null datetime64[ns]\n",
      "Last Pay Status      113509 non-null float64\n",
      "Last Pay Type        113288 non-null object\n",
      "Pay Method           118063 non-null object\n",
      "State                119832 non-null object\n",
      "Status               119832 non-null object\n",
      "ZipCodeNew           119832 non-null float64\n",
      "Stage                100142 non-null object\n",
      "date_created         119832 non-null datetime64[ns]\n",
      "time_created         119832 non-null object\n",
      "days                 113509 non-null float64\n",
      "months               113509 non-null float64\n",
      "year                 119832 non-null float64\n",
      "dtypes: datetime64[ns](2), float64(10), object(8)\n",
      "memory usage: 18.3+ MB\n"
     ]
    }
   ],
   "source": [
    "# Showing the data types for each column\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the row 'Created' as it's redundant\n",
    "data.drop(['Created'], axis=1)\n",
    "\n",
    "# Changing the order of the columns in the dataset\n",
    "data = data[['ID', 'year', 'Agent Label +ID', 'date_created', 'time_created', 'Last Pay Date', 'Pay Method', \n",
    "             'State', 'ZipCodeNew','Last Pay Amount', 'Last Pay Complete', 'Last Pay Status', 'Last Pay Type', \n",
    "             '#Prod', 'Stage', 'Status', 'Income', 'days', 'months']]\n",
    "\n",
    "# Renaming the columns in the data.csv dataset\n",
    "data.columns = ['id', 'year', 'agent', 'date_created', 'time_created', 'lp_date', 'pay_method', 'state', 'zip_code',\n",
    "               'lp_amount', 'lp_complete', 'lp_status', 'lp_type', 'prod', 'stage', 'status', 'income', 'days', 'months']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imputations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 1\n",
      "year 1\n",
      "agent 1\n",
      "date_created 1\n",
      "time_created 1\n",
      "lp_date 6324\n",
      "pay_method 1770\n",
      "state 1\n",
      "zip_code 1\n",
      "lp_amount 6324\n",
      "lp_complete 6324\n",
      "lp_status 6324\n",
      "lp_type 6545\n",
      "prod 39\n",
      "stage 19691\n",
      "status 1\n",
      "income 1309\n",
      "days 6324\n",
      "months 6324\n"
     ]
    }
   ],
   "source": [
    "# Same code we used in Data Mining to calculate number of missing values per column\n",
    "\n",
    "# Creating an empty list for column names\n",
    "names = []\n",
    "\n",
    "# Creating an empty list for the number of null values in each column\n",
    "values = []\n",
    "\n",
    "# Checking for Missing Values\n",
    "for col in data.columns:\n",
    "    names.append(col)\n",
    "    values.append(data[col].isnull().sum())\n",
    "    print(names[-1],values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting customers after the data 5/16/2018\n",
    "# The csv files were created on 5/16/2019 so we want to exclude all the dates that cannot hit the 12 month threshold\n",
    "data = data[~(data['date_created'] > '5/16/2018')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing the income column using the median of the income column\n",
    "# Filling in any null values or 0 income values from the US Household Income dataset\n",
    "data['income'] = data['income'].fillna(data['income'].median())\n",
    "data['income'] = data['income'].replace(0, data['income'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54565, 19)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deleting rows that do not include a Last Pay Date since the customer never paid for a product\n",
    "data = data[pd.notnull(data['lp_date'])]\n",
    "\n",
    "# Deleting one row where the zip code was never given\n",
    "data = data[pd.notnull(data['zip_code'])]\n",
    "\n",
    "# Deleting rows of customers who do not live in the United States\n",
    "data = data[~data['state'].isin(['PR'])]\n",
    "data = data[~data['state'].isin(['NT'])]\n",
    "data = data[~data['state'].isin(['PE'])]\n",
    "data = data[~data['state'].isin(['BC'])]\n",
    "\n",
    "# Removing those customers who did not stay with the company over 1 month\n",
    "# The company would not have received a payment from these customers\n",
    "data = data[~(data['months'] < .99)]\n",
    "data = data[~data['status'].isin(['Switched to LW'])]\n",
    "data = data[~data['stage'].isin(['Quick Cancel'])]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 0\n",
      "year 0\n",
      "agent 0\n",
      "date_created 0\n",
      "time_created 0\n",
      "lp_date 0\n",
      "pay_method 13\n",
      "state 0\n",
      "zip_code 0\n",
      "lp_amount 0\n",
      "lp_complete 0\n",
      "lp_status 0\n",
      "lp_type 19\n",
      "prod 0\n",
      "stage 1873\n",
      "status 0\n",
      "income 0\n",
      "days 0\n",
      "months 0\n"
     ]
    }
   ],
   "source": [
    "# Same code we used in Data Mining to calculate number of missing values per column\n",
    "\n",
    "# Creating an empty list for column names\n",
    "names = []\n",
    "\n",
    "# Creating an empty list for the number of null values in each column\n",
    "values = []\n",
    "\n",
    "# Checking for Missing Values\n",
    "for col in data.columns:\n",
    "    names.append(col)\n",
    "    values.append(data[col].isnull().sum())\n",
    "    print(names[-1],values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['MasterCard' 'Discover' 'Visa' 'ACH' 'American Express' 'Paper_Check'\n",
      " 'Mastercard' nan 'Other' 'Maestro']\n",
      "credit card    45074\n",
      "ACH             8121\n",
      "paper check     1343\n",
      "Other             27\n",
      "Name: pay_method, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Binning categorical columns to larger groups\n",
    "# Printing unique values for columns\n",
    "print(data['pay_method'].unique())\n",
    "\n",
    "#Binning 'pay_method' into 4 groups: Credit card, ACH, Paper Check, & Other\n",
    "data['pay_method'] = data['pay_method'].replace({'Visa': 'credit card', 'MasterCard': 'credit card', \n",
    "                                                 'Discover': 'credit card', 'American Express': 'credit card',\n",
    "                                                'Mastercard': 'credit card', 'Maestro': 'credit card',\n",
    "                                                'Paper_Check': 'paper check', np.nan: 'Other'})  \n",
    "\n",
    "# Counting the number of instances for each value in the 'pay_method' column\n",
    "print(data['pay_method'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a binary code for whether a customer staying over 12 months or not\n",
    "# 1 = greater than 12 months, 0 = less than 12 months\n",
    "data['Y'] = np.where(data['months'] >= 12., 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Returned' 'Order Canceled' 'Returned < 90 days' 'Decline Cancel'\n",
      " 'Active' 'Returned <30 days' 'Decline' 'Returned to Sender' 'Deactivated'\n",
      " 'Restricted' 'Daily Decline' 'Duplicate' 'Returned <60 Days' 'Suspend'\n",
      " 'Collections' 'Chargeback Received' 'Test' 'In Process']\n"
     ]
    }
   ],
   "source": [
    "# Printing out the unique values for the 'status' column to see what values need to be combined\n",
    "print(data['status'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing the status names to more general names\n",
    "data['status'] = data['status'].replace({'Returned <30 days': 'Returned_90', 'Decline Cancel': 'Decline',\n",
    "                                        'Order Canceled': 'Order Cancelled', 'Daily Decline': 'Decline', \n",
    "                                        'Returned <60 Days': 'Returned_90', 'Chargeback Received': 'Other',\n",
    "                                        'In Process': 'Other', 'Post Date': 'Other', 'Suspend': 'Other',\n",
    "                                        'Test': 'Other', 'Returned < 90 days': 'Returned_90'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'year', 'agent', 'date_created', 'time_created', 'lp_date',\n",
       "       'pay_method', 'state', 'zip_code', 'lp_amount', 'lp_complete',\n",
       "       'lp_status', 'lp_type', 'prod', 'stage', 'status', 'income', 'days',\n",
       "       'months', 'Y', 'pay_method_ACH', 'pay_method_Other',\n",
       "       'pay_method_credit card', 'pay_method_paper check'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the method of one-hot encoding for the different forms of pay methods\n",
    "data = pd.concat([data, pd.get_dummies(data['pay_method'], prefix = 'pay_method')], axis = 1)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['MA', 'TX', 'NY', 'ME', 'NC', 'MT', 'RI', 'NH', 'KY', 'VT', 'CT',\n",
       "       'PA', 'NJ', 'NM', 'CA', 'NE', 'VA', 'DE', 'DC', 'MD', 'WV', 'WI',\n",
       "       'TN', 'SC', 'GA', 'FL', 'LA', 'AL', 'MS', 'MI', 'KS', 'MO', 'OH',\n",
       "       'AR', 'IN', 'IA', 'WA', 'MN', 'SD', 'ND', 'IL', 'AZ', 'OK', 'OR',\n",
       "       'CO', 'WY', 'ID', 'UT', 'NV', 'HI', 'AK'], dtype=object)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying all of the states in the 'states' column\n",
    "data.state.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouping states into regions to avoid dimensionality problems when running the data in the models\n",
    "data['state'] = data.state.replace({'MA': 'north_east', 'TX': 'south', 'NY': 'north_east', 'CO': 'rocky', \n",
    "                                    'ME': 'north_east', 'PA': 'north_east', 'MT': 'rocky', 'NJ': 'north_east', \n",
    "                                    'CA': 'west', 'NC': 'south_east', 'RI': 'north_east','SC': 'south_east', \n",
    "                                    'NH': 'north_east', 'GA': 'south_east', 'FL': 'south_east', 'KY': 'central', \n",
    "                                    'OH': 'central', 'MI': 'central', 'VT': 'north_east', 'AL': 'south', 'WI': 'central', \n",
    "                                    'CT': 'north_east','IL': 'central', 'KS': 'central', 'VA': 'north_east', \n",
    "                                    'LA': 'south', 'NM': 'south', 'UT': 'rocky', 'NE': 'rocky', \n",
    "                                    'DE': 'north_east', 'DC': 'north_east', 'WA': 'west','MD': 'north_east', \n",
    "                                    'MN': 'central', 'NV': 'west', 'WV': 'central', 'AZ': 'west', 'TN': 'central',\n",
    "                                    'MS': 'south', 'HI': 'west', 'IN': 'central', 'MO': 'south', 'AK': 'west', \n",
    "                                    'AR': 'south', 'IA': 'central', 'ID': 'rocky', 'WY': 'rocky', 'SD': 'rocky', \n",
    "                                    'ND': 'rocky', 'OK': 'south', 'OR': 'west'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "north_east    12173\n",
       "south         11839\n",
       "south_east    10706\n",
       "central       10637\n",
       "west           7528\n",
       "rocky          1682\n",
       "Name: state, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prinitng out the new state bins to make sure no states were omitted in the replace dictionary\n",
    "data.state.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'year', 'agent', 'date_created', 'time_created', 'lp_date',\n",
       "       'pay_method', 'state', 'zip_code', 'lp_amount', 'lp_complete',\n",
       "       'lp_status', 'lp_type', 'prod', 'stage', 'status', 'income', 'days',\n",
       "       'months', 'Y', 'pay_method_ACH', 'pay_method_Other',\n",
       "       'pay_method_credit card', 'pay_method_paper check', 'region_central',\n",
       "       'region_north_east', 'region_rocky', 'region_south',\n",
       "       'region_south_east', 'region_west'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the method of one-hot encoding for the different regions\n",
    "data = pd.concat([data, pd.get_dummies(data['state'], prefix = 'region')], axis = 1)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'year', 'agent', 'date_created', 'time_created', 'lp_date',\n",
       "       'pay_method', 'state', 'zip_code', 'lp_amount', 'lp_complete',\n",
       "       'lp_status', 'lp_type', 'prod', 'stage', 'status', 'income', 'days',\n",
       "       'months', 'Y', 'pay_method_ACH', 'pay_method_Other',\n",
       "       'pay_method_credit card', 'pay_method_paper check', 'region_central',\n",
       "       'region_north_east', 'region_rocky', 'region_south',\n",
       "       'region_south_east', 'region_west', 'status_Active',\n",
       "       'status_Collections', 'status_Deactivated', 'status_Decline',\n",
       "       'status_Duplicate', 'status_Order Cancelled', 'status_Other',\n",
       "       'status_Restricted', 'status_Returned', 'status_Returned to Sender',\n",
       "       'status_Returned_90'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the method of one-hot encoding for the different status types\n",
    "data = pd.concat([data, pd.get_dummies(data['status'], prefix = 'status')], axis = 1)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['year', 'agent', 'pay_method', 'state', 'zip_code', 'status', 'income',\n",
       "       'months', 'Y', 'pay_method_ACH', 'pay_method_credit card',\n",
       "       'pay_method_paper check', 'region_central', 'region_north_east',\n",
       "       'region_rocky', 'region_south', 'region_south_east', 'region_west',\n",
       "       'status_Active', 'status_Decline', 'status_Returned',\n",
       "       'status_Returned_90'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dropping columns we will not be using in the models\n",
    "data = data.drop(['id', 'date_created', 'time_created', 'lp_date','days', 'lp_complete', 'lp_status', 'lp_type', \n",
    "                  'prod', 'stage', 'lp_amount',  'status_Collections', 'status_Deactivated', 'status_Duplicate', \n",
    "                  'status_Order Cancelled', 'status_Other', 'status_Restricted', 'status_Returned to Sender', \n",
    "                  'pay_method_Other'], axis = 1)\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      year                              agent   pay_method       state  \\\n",
      "7   2017.0             Yasha - Pers1 (264925)  credit card  north_east   \n",
      "13  2015.0  Benefits and More 4 Paid (143595)  credit card  north_east   \n",
      "16  2014.0                 Newspaper (109455)  credit card  north_east   \n",
      "17  2014.0                  Hospital (109465)  credit card  north_east   \n",
      "19  2015.0  Benefits and More 4 Paid (143595)  credit card  north_east   \n",
      "\n",
      "    zip_code           status   income  months  Y  pay_method_ACH  \\\n",
      "7     1002.0         Returned  74083.0    3.73  0               0   \n",
      "13    1010.0         Returned  51250.0   13.67  1               0   \n",
      "16    1013.0  Order Cancelled  29243.0    2.07  0               0   \n",
      "17    1013.0      Returned_90  29243.0    3.00  0               0   \n",
      "19    1013.0         Returned  29243.0   13.57  1               0   \n",
      "\n",
      "           ...          region_central  region_north_east  region_rocky  \\\n",
      "7          ...                       0                  1             0   \n",
      "13         ...                       0                  1             0   \n",
      "16         ...                       0                  1             0   \n",
      "17         ...                       0                  1             0   \n",
      "19         ...                       0                  1             0   \n",
      "\n",
      "    region_south  region_south_east  region_west  status_Active  \\\n",
      "7              0                  0            0              0   \n",
      "13             0                  0            0              0   \n",
      "16             0                  0            0              0   \n",
      "17             0                  0            0              0   \n",
      "19             0                  0            0              0   \n",
      "\n",
      "    status_Decline  status_Returned  status_Returned_90  \n",
      "7                0                1                   0  \n",
      "13               0                1                   0  \n",
      "16               0                0                   0  \n",
      "17               0                0                   1  \n",
      "19               0                1                   0  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "(54565, 22)\n"
     ]
    }
   ],
   "source": [
    "# Calling data.head() to view the data and the dimensions of the new dataset\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year 0\n",
      "agent 0\n",
      "pay_method 0\n",
      "state 0\n",
      "zip_code 0\n",
      "status 0\n",
      "income 0\n",
      "months 0\n",
      "Y 0\n",
      "pay_method_ACH 0\n",
      "pay_method_credit card 0\n",
      "pay_method_paper check 0\n",
      "region_central 0\n",
      "region_north_east 0\n",
      "region_rocky 0\n",
      "region_south 0\n",
      "region_south_east 0\n",
      "region_west 0\n",
      "status_Active 0\n",
      "status_Decline 0\n",
      "status_Returned 0\n",
      "status_Returned_90 0\n"
     ]
    }
   ],
   "source": [
    "# Same code we used in Data Mining to calculate number of missing values per column\n",
    "\n",
    "# Creating an empty list for column names\n",
    "names = []\n",
    "\n",
    "# Creating an empty list for the number of null values in each column\n",
    "values = []\n",
    "\n",
    "# Checking for Missing Values\n",
    "for col in data.columns:\n",
    "    names.append(col)\n",
    "    values.append(data[col].isnull().sum())\n",
    "    print(names[-1],values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the imputed data as impute_data.csv to begin normalization\n",
    "data.to_csv('imputed_data.csv',',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
